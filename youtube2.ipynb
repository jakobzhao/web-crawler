{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "mDt4G3hZCp9J"
      },
      "outputs": [],
      "source": [
        "# created on April 14, 2021\n",
        "# modified on Jan 2, 2022\n",
        "# @author:          Bo Zhao\n",
        "# @email:           zhaobo@uw.edu\n",
        "# @website:         https://hgis.uw.edu\n",
        "# @organization:    Department of Geography, University of Washington, Seattle\n",
        "# @description:     A demo of collecting data from YouTube."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "9dXwu7Ay9Twv"
      },
      "outputs": [],
      "source": [
        "# Installing Kora to the remote google colab server. Kora is a collection of tools to make programming on Google Colab easier.\n",
        "!pip install kora -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Fxx5mNyHC0-0"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import time, datetime\n",
        "import pandas as pd\n",
        "# A bot can automate the collecting data process. A bot will imitate how an user browse a web page, and then acquire those useful information.\n",
        "# For more information about how to operate a bot, please refer to the documentation of Selenium at https://selenium-python.readthedocs.io/\n",
        "from kora.selenium import wd as bot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSErhAJ1C50o"
      },
      "outputs": [],
      "source": [
        "# The url where the data will be collected from.\n",
        "url = \"https://www.youtube.com/results?search_query=standing+rock\"\n",
        "\n",
        "# Input the targeting url to the bot, and the bot will load data from the url.\n",
        "bot.get(url)\n",
        "\n",
        "# An array to store all the video urls. If a video has been crawled, it would not be stored to the data frame.\n",
        "video_urls = []\n",
        "# An array to store the retrieved video details.\n",
        "results = []\n",
        "\n",
        "\n",
        "# variable i indicates the number of times that scrolls down a web page. In practice, you might want to develop different\n",
        "# interaction approach to load and view the web pages.\n",
        "\n",
        "for i in range(5):\n",
        "\n",
        "    # Create a document object model (DOM) from the raw source of the crawled web page.\n",
        "    # Since you are processing a html page, 'html.parser' is chosen.\n",
        "    soup = BeautifulSoup(bot.page_source, 'html.parser')\n",
        "\n",
        "    # Capture all the video items using find_all or findAll method.\n",
        "    # To view the information of the html elements you want to collect, you need to inspect the raw source using Chrome Inspector.\n",
        "    # To test whether you find the right html elements, you can use the pycharm debugger to examine the returned data.\n",
        "    videos = soup.find_all('ytd-video-renderer', class_=\"style-scope ytd-item-section-renderer\")[-20:] # 20 indicates only process the newly-acquired 20 entries.\n",
        "\n",
        "    # iterate and process each video entry.\n",
        "    for video in videos:\n",
        "\n",
        "        # I prefer use the \"try-except\" statement to enable the program run without pausing due to unexpected errors.\n",
        "        try:\n",
        "            video_url = video.find(\"a\", class_=\"yt-simple-endpoint inline-block style-scope ytd-thumbnail\").attrs[\"href\"]\n",
        "            user_url = video.find(\"a\", class_=\"yt-simple-endpoint style-scope yt-formatted-string\").attrs[\"href\"]\n",
        "            username = video.find(\"a\", class_=\"yt-simple-endpoint style-scope yt-formatted-string\").text\n",
        "            title = video.find(\"yt-formatted-string\", class_=\"style-scope ytd-video-renderer\").text\n",
        "            view_num = video.find_all(\"span\", class_=\"style-scope ytd-video-meta-block\")[0].text.replace(\" views\", \"\")\n",
        "            created_at = video.find_all(\"span\", class_=\"style-scope ytd-video-meta-block\")[0].text.replace(\" ago\", \"\")\n",
        "            shortdesc = video.find(\"yt-formatted-string\", id=\"description-text\").text\n",
        "            collected_at = datetime.datetime.now()\n",
        "\n",
        "            # create a row in the dict format.\n",
        "            row = {'video_url': video_url,\n",
        "                    'user_url': user_url,\n",
        "                    'username': username,\n",
        "                    'title': title,\n",
        "                    'view_num': view_num,\n",
        "                    'created_at': created_at,\n",
        "                    'shortdesc': shortdesc,\n",
        "                    'collected_at': collected_at}\n",
        "\n",
        "            # if a video has been added, this video would not be inserted to the results array,\n",
        "            # otherwise, this video will be inserted.\n",
        "            if video_url in video_urls:\n",
        "                print(\"this video has already been added.\")\n",
        "            else:\n",
        "                print(row)\n",
        "                video_urls.append(video_url)\n",
        "                results.append(row)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "    # it is very important to enable the bot take some rest, and then resume to work.\n",
        "    time.sleep(5)\n",
        "\n",
        "    # Let the bot scrolls down to the bottom of the content element, most of the time the bot needs to scroll down to the bottom of the page.\n",
        "    # like this statement: bot.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "    bot.execute_script('window.scrollTo(0,  document.getElementById(\"content\").scrollHeight);')\n",
        "\n",
        "# terminate the bot object.\n",
        "bot.close()\n",
        "\n",
        "# Store the results as a pandas dataframe\n",
        "df = pd.DataFrame(results)\n",
        "\n",
        "# notify the completion of the crawling in the console.\n",
        "print(\"the crawling task is finished.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "pxJng3mfXT3I"
      },
      "outputs": [],
      "source": [
        "# Create data on to Google Drive\n",
        "from google.colab import drive\n",
        "# Mount your Drive to the Colab VM.\n",
        "#drive._mount('/gdrive')\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "# the file path where to store the output csv on google drive\n",
        "output_file = '/gdrive/My Drive/videos.csv'\n",
        "\n",
        "# Save the dataframe as a csv file\n",
        "df.to_csv(output_file, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3-zJPqIRb7w"
      },
      "outputs": [],
      "source": [
        "# download the csv to your local computer\n",
        "from google.colab import files\n",
        "files.download(output_file)\n",
        "print(\"the csv has been downloaded to your local computer. The program has been completed successfully.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "youtube.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
